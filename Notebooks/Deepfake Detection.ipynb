{"cells":[{"metadata":{},"cell_type":"markdown","source":"Transfer Learning using VGG16","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.applications.vgg16 import VGG16\nfrom keras.layers import Dense, LSTM, Flatten, TimeDistributed, Dropout\nfrom keras.models import Sequential, Model\n\ndef df_detection_model_crnn(inp_shape):\n    # using transfer learning from vgg16\n    vgg = VGG16(weights='imagenet',include_top=False, input_shape=(224,224,3))\n    \n    for layer in vgg.layers:\n        layer.trainable = True\n    \n    model = Sequential()\n    \n    \n    model.add(TimeDistributed(vgg.layers[0],input_shape=inp_shape))\n    \n    for i in range(1,len(vgg.layers)):\n        model.add(TimeDistributed(vgg.layers[i]))\n\n        \n    #model.add(TimeDistributed(vgg,input_shape=inp_shape))  # (5,224,224,3) -> (5,7,7,512)\n    model.add(TimeDistributed(Flatten()))   # (5,7,7,512) -> (512*7*7*5)\n    model.add(LSTM(512, activation='tanh',return_sequences=True))\n    model.add(LSTM(256, activation='tanh'))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(32,activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(2,activation='softmax'))\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transfer Learning using VGG-16 BiLSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.applications.vgg16 import VGG16\nfrom keras.layers import Dense, LSTM, Flatten, TimeDistributed, Dropout, Bidirectional\nfrom keras.models import Sequential, Model\n\ndef df_detection_model_crnn(inp_shape):\n    # using transfer learning from vgg16\n    vgg = VGG16(weights='imagenet',include_top=False, input_shape=(224,224,3))\n    \n    for layer in vgg.layers:\n        layer.trainable = True\n    \n    model = Sequential()\n    \n    \n    model.add(TimeDistributed(vgg.layers[0],input_shape=inp_shape))\n    \n    for i in range(1,len(vgg.layers)):\n        model.add(TimeDistributed(vgg.layers[i]))\n\n        \n    #model.add(TimeDistributed(vgg,input_shape=inp_shape))  # (30,224,224,3) -> (30,7,7,512)\n    model.add(TimeDistributed(Flatten()))   # (30,7,7,512) -> (512)\n    model.add(Bidirectional(LSTM(512, activation='tanh',return_sequences=True)))\n    model.add(Bidirectional(LSTM(256, activation='tanh')))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(32,activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(2,activation='softmax'))\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transfer Learning using VGG-19","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.applications.vgg19 import VGG19\nfrom keras.layers import Dense, LSTM, Flatten, TimeDistributed, Dropout\nfrom keras.models import Sequential, Model\n\ndef df_detection_model_crnn(inp_shape):\n    # using transfer learning from vgg19\n    vgg = VGG19(weights='imagenet',include_top=False, input_shape=(224,224,3))\n    \n    for layer in vgg.layers:\n        layer.trainable = True\n    \n    model = Sequential()\n    \n    \n    model.add(TimeDistributed(vgg.layers[0],input_shape=inp_shape))\n    \n    for i in range(1,len(vgg.layers)):\n        model.add(TimeDistributed(vgg.layers[i]))\n\n        \n    #model.add(TimeDistributed(vgg,input_shape=inp_shape))  # (30,224,224,3) -> (30,7,7,512)\n    model.add(TimeDistributed(Flatten()))   # (30,7,7,512) -> (512)\n    model.add(LSTM(512, activation='tanh',return_sequences=True))\n    model.add(LSTM(256, activation='tanh'))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(32,activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(2,activation='softmax'))\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Transfer Learning using VGG19 BiLSTM","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nfrom keras.applications.vgg19 import VGG19\nfrom keras.layers import Dense, LSTM, Flatten, TimeDistributed, Dropout, Bidirectional\nfrom keras.models import Sequential, Model\n\ndef df_detection_model_crnn(inp_shape):\n    # using transfer learning from vgg19\n    vgg = VGG19(weights='imagenet',include_top=False, input_shape=(224,224,3))\n    \n    for layer in vgg.layers:\n        layer.trainable = True\n    \n    model = Sequential()\n    \n    \n    model.add(TimeDistributed(vgg.layers[0],input_shape=inp_shape))\n    \n    for i in range(1,len(vgg.layers)):\n        model.add(TimeDistributed(vgg.layers[i]))\n\n        \n    #model.add(TimeDistributed(vgg,input_shape=inp_shape))  # (30,224,224,3) -> (30,7,7,512)\n    model.add(TimeDistributed(Flatten()))   # (30,7,7,512) -> (512)\n    model.add(Bidirectional(LSTM(512, activation='tanh',return_sequences=True)))\n    model.add(Bidirectional(LSTM(256, activation='tanh')))\n    model.add(Dense(256, activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(32,activation='relu'))\n    model.add(Dropout(.3))\n    model.add(Dense(2,activation='softmax'))\n    model.summary()\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%tensorboard --logdir=logs/","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install keras-video-generators\nimport glob\nimport numpy as np\nimport keras\nimport tensorflow as tf\nimport psutil\nfrom keras_video import VideoFrameGenerator\nimport keras_video.utils\nimport os\nimport matplotlib.pyplot as plt\nfrom time import time\nfrom keras.callbacks import TensorBoard\n\nclasses = [i.split(os.path.sep)[4] for i in glob.glob('../input/deepfake-dataset/Celeb-DF-v2/*')]\nclasses.sort()\nprint(classes)\n\nTSHAPE = (224,224)\nCHANNELS = 3\nNBFRAME = 5\nBS = 10\n# pattern to get videos and classes\nglob_pattern='../input/deepfake-dataset/Celeb-DF-v2/{classname}/*.mp4'\n\ntrain = VideoFrameGenerator(\n    classes=classes, \n    glob_pattern=glob_pattern,\n    nb_frames=NBFRAME,\n    split_val=.1,\n    split_test=.4,\n    shuffle=True,\n    batch_size=BS,\n    target_shape=TSHAPE,\n    nb_channel=CHANNELS,\n    use_frame_cache=False)\n\nvalid = train.get_validation_generator()\n\ntest = train.get_test_generator()\n\nkeras_video.utils.show_sample(train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import backend as K\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives / (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"INPSHAPE = (NBFRAME,224,224,3)\n\nmodel = df_detection_model_crnn(INPSHAPE)\n#model.load_weights('../input/weights-batch-3/weights_batch3.hdf5')\n\nMETRICS = [\n    keras.metrics.CategoricalAccuracy(name='categorical_accuracy')\n]\n\noptimizer = keras.optimizers.SGD(lr = 0.001)\nloss = keras.losses.CategoricalCrossentropy()\nmodel.compile(\n    optimizer,\n    loss=loss,\n    metrics=METRICS\n)\n\nEPOCHS=100\n\n# callbacks = [\n#     keras.callbacks.ReduceLROnPlateau(verbose=1),\n#     keras.callbacks.ModelCheckpoint(\n#         '/kaggle/working/weights_batch4.hdf5',\n#         verbose=1,save_best_only=True),\n# ]\n\n#tensorboard_callback = TensorBoard(log_dir='logs/{}'.format(time()))\n\nhistory = model.fit_generator(\n    train,\n    validation_data = valid,\n    verbose=1,\n    epochs=EPOCHS,\n    shuffle=True\n)\n\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['categorical_accuracy'])\nplt.plot(history.history['val_categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"output = model.evaluate_generator(test,verbose=1)\nprint(output)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('/kaggle/working/final_model_vgg16_bilstm.h5')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}